{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "intro"
      },
      "source": [
        "# üß† Generador IA de bancos Moodle XML + Equilibrado y PDF\n",
        "\n",
        "Este cuaderno:\n",
        "1) **Sube un documento** base (PDF/DOCX/TXT).\n",
        "2) La IA **genera preguntas MCQ** (autocontenidas, 1 correcta + 2 distractores plausibles).\n",
        "3) **Equilibra** longitudes de opciones (¬±4 palabras) y **baraja** opciones.\n",
        "4) Exporta **Moodle XML** listo para importar y un **PDF** de revisi√≥n (‚úÖ en la correcta).\n",
        "\n",
        "**Uso:** ejecuta las celdas en orden. Cuando pida *subir archivo*, selecciona tu documento fuente.\n"
      ],
      "id": "intro"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install"
      },
      "outputs": [],
      "source": [
        "!pip install openai==1.* beautifulsoup4 lxml reportlab pypdf python-docx tqdm --quiet"
      ],
      "id": "install"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "apikey",
        "outputId": "fd193d02-6683-4221-b329-243716d23651",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ API key configurada en el entorno de ejecuci√≥n.\n"
          ]
        }
      ],
      "source": [
        "# üîë Establece tu API key de OpenAI de forma segura (no queda guardada en el cuaderno)\n",
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "if not os.getenv(\"OPENAI_API_KEY\"):\n",
        "    os.environ[\"OPENAI_API_KEY\"] = getpass(\"Pega tu OPENAI_API_KEY y pulsa Enter: \")\n",
        "print(\"‚úÖ API key configurada en el entorno de ejecuci√≥n.\")"
      ],
      "id": "apikey"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pipeline",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9f9f4689-3b07-4c9b-a8ce-1ffe98eb9877"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìÅ Sube tu documento base (PDF/DOCX/TXT)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-4a30d37c-b634-43c9-a435-ad519815b9ed\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-4a30d37c-b634-43c9-a435-ad519815b9ed\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Neurociencia-del-lenguaje.pdf to Neurociencia-del-lenguaje (2).pdf\n",
            "‚úÖ Cargado: Neurociencia-del-lenguaje (2).pdf\n",
            "üß© Bloques de texto creados: 14 (‚âà6000 chars cada uno)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generando √≠tems IA:   0%|          | 0/14 [00:00<?, ?it/s]Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-2654049910.py\", line 243, in <cell line: 0>\n",
            "    items = llm_items_from_text(ch, block_id=f\"B{bi}\", n=ITEMS_PER_BLOCK)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-2654049910.py\", line 128, in llm_items_from_text\n",
            "    resp = client.chat.completions.create(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_utils/_utils.py\", line 286, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py\", line 1147, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\", line 1259, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\", line 1047, in request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n",
            "Generando √≠tems IA:   7%|‚ñã         | 1/14 [00:03<00:46,  3.60s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚ö†Ô∏è Bloque 1: error de generaci√≥n ‚Üí Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-2654049910.py\", line 243, in <cell line: 0>\n",
            "    items = llm_items_from_text(ch, block_id=f\"B{bi}\", n=ITEMS_PER_BLOCK)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-2654049910.py\", line 128, in llm_items_from_text\n",
            "    resp = client.chat.completions.create(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_utils/_utils.py\", line 286, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py\", line 1147, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\", line 1259, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\", line 1047, in request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n",
            "\rGenerando √≠tems IA:  14%|‚ñà‚ñç        | 2/14 [00:05<00:34,  2.85s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚ö†Ô∏è Bloque 2: error de generaci√≥n ‚Üí Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-2654049910.py\", line 243, in <cell line: 0>\n",
            "    items = llm_items_from_text(ch, block_id=f\"B{bi}\", n=ITEMS_PER_BLOCK)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-2654049910.py\", line 128, in llm_items_from_text\n",
            "    resp = client.chat.completions.create(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_utils/_utils.py\", line 286, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py\", line 1147, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\", line 1259, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\", line 1047, in request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n",
            "\rGenerando √≠tems IA:  21%|‚ñà‚ñà‚ñè       | 3/14 [00:08<00:29,  2.68s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚ö†Ô∏è Bloque 3: error de generaci√≥n ‚Üí Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-2654049910.py\", line 243, in <cell line: 0>\n",
            "    items = llm_items_from_text(ch, block_id=f\"B{bi}\", n=ITEMS_PER_BLOCK)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-2654049910.py\", line 128, in llm_items_from_text\n",
            "    resp = client.chat.completions.create(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_utils/_utils.py\", line 286, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py\", line 1147, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\", line 1259, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\", line 1047, in request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n",
            "\rGenerando √≠tems IA:  29%|‚ñà‚ñà‚ñä       | 4/14 [00:10<00:24,  2.48s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚ö†Ô∏è Bloque 4: error de generaci√≥n ‚Üí Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-2654049910.py\", line 243, in <cell line: 0>\n",
            "    items = llm_items_from_text(ch, block_id=f\"B{bi}\", n=ITEMS_PER_BLOCK)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-2654049910.py\", line 128, in llm_items_from_text\n",
            "    resp = client.chat.completions.create(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_utils/_utils.py\", line 286, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py\", line 1147, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\", line 1259, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\", line 1047, in request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n",
            "\rGenerando √≠tems IA:  36%|‚ñà‚ñà‚ñà‚ñå      | 5/14 [00:12<00:21,  2.41s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚ö†Ô∏è Bloque 5: error de generaci√≥n ‚Üí Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-2654049910.py\", line 243, in <cell line: 0>\n",
            "    items = llm_items_from_text(ch, block_id=f\"B{bi}\", n=ITEMS_PER_BLOCK)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-2654049910.py\", line 128, in llm_items_from_text\n",
            "    resp = client.chat.completions.create(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_utils/_utils.py\", line 286, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py\", line 1147, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\", line 1259, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\", line 1047, in request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n",
            "\rGenerando √≠tems IA:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 6/14 [00:15<00:20,  2.52s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚ö†Ô∏è Bloque 6: error de generaci√≥n ‚Üí Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-2654049910.py\", line 243, in <cell line: 0>\n",
            "    items = llm_items_from_text(ch, block_id=f\"B{bi}\", n=ITEMS_PER_BLOCK)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-2654049910.py\", line 128, in llm_items_from_text\n",
            "    resp = client.chat.completions.create(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_utils/_utils.py\", line 286, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py\", line 1147, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\", line 1259, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\", line 1047, in request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n",
            "\rGenerando √≠tems IA:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 7/14 [00:17<00:15,  2.25s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚ö†Ô∏è Bloque 7: error de generaci√≥n ‚Üí Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-2654049910.py\", line 243, in <cell line: 0>\n",
            "    items = llm_items_from_text(ch, block_id=f\"B{bi}\", n=ITEMS_PER_BLOCK)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-2654049910.py\", line 128, in llm_items_from_text\n",
            "    resp = client.chat.completions.create(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_utils/_utils.py\", line 286, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py\", line 1147, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\", line 1259, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\", line 1047, in request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n",
            "\rGenerando √≠tems IA:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 8/14 [00:20<00:15,  2.58s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚ö†Ô∏è Bloque 8: error de generaci√≥n ‚Üí Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-2654049910.py\", line 243, in <cell line: 0>\n",
            "    items = llm_items_from_text(ch, block_id=f\"B{bi}\", n=ITEMS_PER_BLOCK)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-2654049910.py\", line 128, in llm_items_from_text\n",
            "    resp = client.chat.completions.create(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_utils/_utils.py\", line 286, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py\", line 1147, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\", line 1259, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\", line 1047, in request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n",
            "\rGenerando √≠tems IA:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 9/14 [00:22<00:11,  2.27s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚ö†Ô∏è Bloque 9: error de generaci√≥n ‚Üí Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-2654049910.py\", line 243, in <cell line: 0>\n",
            "    items = llm_items_from_text(ch, block_id=f\"B{bi}\", n=ITEMS_PER_BLOCK)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-2654049910.py\", line 128, in llm_items_from_text\n",
            "    resp = client.chat.completions.create(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_utils/_utils.py\", line 286, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py\", line 1147, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\", line 1259, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\", line 1047, in request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n",
            "\rGenerando √≠tems IA:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 10/14 [00:23<00:08,  2.10s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚ö†Ô∏è Bloque 10: error de generaci√≥n ‚Üí Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-2654049910.py\", line 243, in <cell line: 0>\n",
            "    items = llm_items_from_text(ch, block_id=f\"B{bi}\", n=ITEMS_PER_BLOCK)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-2654049910.py\", line 128, in llm_items_from_text\n",
            "    resp = client.chat.completions.create(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_utils/_utils.py\", line 286, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py\", line 1147, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\", line 1259, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\", line 1047, in request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n",
            "\rGenerando √≠tems IA:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 11/14 [00:25<00:05,  1.92s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚ö†Ô∏è Bloque 11: error de generaci√≥n ‚Üí Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-2654049910.py\", line 243, in <cell line: 0>\n",
            "    items = llm_items_from_text(ch, block_id=f\"B{bi}\", n=ITEMS_PER_BLOCK)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-2654049910.py\", line 128, in llm_items_from_text\n",
            "    resp = client.chat.completions.create(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_utils/_utils.py\", line 286, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py\", line 1147, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\", line 1259, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\", line 1047, in request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n",
            "\rGenerando √≠tems IA:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 12/14 [00:27<00:03,  1.86s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚ö†Ô∏è Bloque 12: error de generaci√≥n ‚Üí Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-2654049910.py\", line 243, in <cell line: 0>\n",
            "    items = llm_items_from_text(ch, block_id=f\"B{bi}\", n=ITEMS_PER_BLOCK)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-2654049910.py\", line 128, in llm_items_from_text\n",
            "    resp = client.chat.completions.create(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_utils/_utils.py\", line 286, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py\", line 1147, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\", line 1259, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\", line 1047, in request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n",
            "\rGenerando √≠tems IA:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 13/14 [00:28<00:01,  1.76s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚ö†Ô∏è Bloque 13: error de generaci√≥n ‚Üí Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-2654049910.py\", line 243, in <cell line: 0>\n",
            "    items = llm_items_from_text(ch, block_id=f\"B{bi}\", n=ITEMS_PER_BLOCK)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-2654049910.py\", line 128, in llm_items_from_text\n",
            "    resp = client.chat.completions.create(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_utils/_utils.py\", line 286, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py\", line 1147, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\", line 1259, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\", line 1047, in request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n",
            "Generando √≠tems IA: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 14/14 [00:30<00:00,  2.17s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚ö†Ô∏è Bloque 14: error de generaci√≥n ‚Üí Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n",
            "‚úÖ √çtems v√°lidos totales: 0\n",
            "üì¶ XML: equilibrado_IA.xml\n",
            "üìÑ PDF: equilibrado_IA.pdf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_929655ed-6fdc-40a5-97e9-d462d92393f1\", \"equilibrado_IA.xml\", 47)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_d53f09e6-10bf-4e72-ab9d-46dd4b972ed9\", \"equilibrado_IA.pdf\", 1625)"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# =========================\n",
        "#  Generar √≠tems con IA + equilibrar + exportar XML + PDF\n",
        "# =========================\n",
        "import os, json, random, re, traceback\n",
        "from tqdm import tqdm\n",
        "from pypdf import PdfReader\n",
        "from docx import Document\n",
        "from google.colab import files\n",
        "from bs4 import BeautifulSoup\n",
        "from bs4.element import Tag\n",
        "from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer\n",
        "from reportlab.lib.pagesizes import A4\n",
        "from reportlab.lib.styles import getSampleStyleSheet\n",
        "from lxml import etree\n",
        "from openai import OpenAI\n",
        "\n",
        "# ===== Par√°metros =====\n",
        "THRESH_DIFF = 4            # diferencia m√°xima de palabras entre correcta e incorrectas\n",
        "RANDOM_SEED = 42           # usa None para aleatoriedad no determinista\n",
        "ITEMS_PER_BLOCK = 6        # preguntas IA por bloque de texto\n",
        "CHUNK_MAX_CHARS = 6000     # tama√±o aprox. de cada bloque del documento fuente\n",
        "OPENAI_MODEL = \"gpt-4o-mini\"  # modelo de generaci√≥n\n",
        "\n",
        "if RANDOM_SEED is not None:\n",
        "    random.seed(RANDOM_SEED)\n",
        "\n",
        "# ===== Carga de documento base =====\n",
        "print(\"üìÅ Sube tu documento base (PDF/DOCX/TXT)\")\n",
        "up = files.upload()\n",
        "SRC = list(up.keys())[0]\n",
        "print(f\"‚úÖ Cargado: {SRC}\")\n",
        "\n",
        "def load_text(path):\n",
        "    p = path.lower()\n",
        "    if p.endswith(\".pdf\"):\n",
        "        reader = PdfReader(path)\n",
        "        return \"\\n\".join([(page.extract_text() or \"\") for page in reader.pages])\n",
        "    if p.endswith(\".docx\"):\n",
        "        doc = Document(path)\n",
        "        return \"\\n\".join([para.text for para in doc.paragraphs])\n",
        "    with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
        "        return f.read()\n",
        "\n",
        "raw_text = load_text(SRC)\n",
        "assert raw_text.strip(), \"El documento parece vac√≠o o no se pudo extraer texto.\"\n",
        "\n",
        "def chunk_text(text, max_chars=10000):\n",
        "    paras = [p.strip() for p in text.split(\"\\n\") if p.strip()]\n",
        "    chunks, cur = [], \"\"\n",
        "    for p in paras:\n",
        "        if len(cur) + len(p) + 1 <= max_chars:\n",
        "            cur += (\"\\n\" + p) if cur else p\n",
        "        else:\n",
        "            chunks.append(cur); cur = p\n",
        "    if cur: chunks.append(cur)\n",
        "    return chunks\n",
        "\n",
        "chunks = chunk_text(raw_text, max_chars=CHUNK_MAX_CHARS)\n",
        "print(f\"üß© Bloques de texto creados: {len(chunks)} (‚âà{CHUNK_MAX_CHARS} chars cada uno)\")\n",
        "\n",
        "# ===== Cliente OpenAI =====\n",
        "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
        "\n",
        "SYSTEM_PROMPT = (\n",
        "    \"Eres un generador de √≠tems universitarios, preciso, en espa√±ol, y devuelves JSON v√°lido.\"\n",
        ")\n",
        "\n",
        "USER_PROMPT_TMPL = \"\"\"\n",
        "Eres un generador de preguntas universitarias en psicolog√≠a del lenguaje/lectura.\n",
        "Crea {n} preguntas tipo test (MCQ) AUTOCONTENIDAS a partir del CONTENIDO. Nivel: universitario.\n",
        "- 1 correcta + 2 distractores plausibles (sin 'todas las anteriores' / 'ninguna').\n",
        "- Redacci√≥n clara, sin ambig√ºedad, NO dependas de \"seg√∫n el texto\".\n",
        "- Incluye justificaci√≥n breve (1‚Äì2 frases) para la correcta.\n",
        "- RESPONDE √öNICAMENTE con JSON. NO incluyas explicaciones, NO uses ```json ni ning√∫n fence.\n",
        "\n",
        "Estructura EXACTA:\n",
        "{{\n",
        "  \"items\": [\n",
        "    {{\n",
        "      \"id\": \"BLOQUE1-Q1\",\n",
        "      \"stem\": \"ENUNCIADO AUTOCONTENIDO...\",\n",
        "      \"options\": [\"A...\", \"B...\", \"C...\"],\n",
        "      \"correct_index\": 1,\n",
        "      \"justification\": \"Por qu√© es correcta...\",\n",
        "      \"difficulty\": \"media\",\n",
        "      \"tags\": [\"efectos de priming\",\"l√©xico\"]\n",
        "    }}\n",
        "  ]\n",
        "}}\n",
        "\n",
        "CONTENIDO:\n",
        "{content}\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def wc(s):\n",
        "    s = re.sub(r\"\\s+\", \" \", s or \"\").strip()\n",
        "    return len([w for w in s.split(\" \") if w])\n",
        "\n",
        "import json, re\n",
        "\n",
        "def _strip_code_fences(s: str) -> str:\n",
        "    # quita ```json ... ``` o ``` ... ```\n",
        "    s = s.strip()\n",
        "    fence = re.compile(r\"^```(?:json)?\\s*([\\s\\S]*?)\\s*```$\", re.IGNORECASE)\n",
        "    m = fence.match(s)\n",
        "    return m.group(1).strip() if m else s\n",
        "\n",
        "def _extract_json_object(s: str) -> str:\n",
        "    # intenta extraer el primer objeto { ... } bien balanceado\n",
        "    start = s.find(\"{\")\n",
        "    if start == -1:\n",
        "        return s\n",
        "    depth = 0\n",
        "    for i, ch in enumerate(s[start:], start=start):\n",
        "        if ch == \"{\":\n",
        "            depth += 1\n",
        "        elif ch == \"}\":\n",
        "            depth -= 1\n",
        "            if depth == 0:\n",
        "                return s[start:i+1]\n",
        "    return s  # si no balancea, devolvemos original\n",
        "\n",
        "def llm_items_from_text(content, block_id=\"B1\", n=6, debug=False):\n",
        "    prompt = USER_PROMPT_TMPL.format(content=content, n=n)\n",
        "    resp = client.chat.completions.create(\n",
        "        model=OPENAI_MODEL,\n",
        "        messages=[\n",
        "            {\"role\":\"system\",\"content\":SYSTEM_PROMPT},\n",
        "            {\"role\":\"user\",\"content\":prompt}\n",
        "        ],\n",
        "        temperature=0.4,\n",
        "        # si tu cuenta no soporta response_format, quita la l√≠nea siguiente\n",
        "        response_format={\"type\":\"json_object\"}\n",
        "    )\n",
        "\n",
        "    raw = resp.choices[0].message.content\n",
        "    try:\n",
        "        # intento 1: JSON directo\n",
        "        data = json.loads(raw)\n",
        "    except Exception:\n",
        "        # intento 2: quitar fences\n",
        "        cleaned = _strip_code_fences(raw)\n",
        "        # intento 3: extraer el objeto { ... } balanceado\n",
        "        cleaned = _extract_json_object(cleaned)\n",
        "        if debug:\n",
        "            print(\"DEBUG raw:\", raw[:400])\n",
        "            print(\"DEBUG cleaned:\", cleaned[:400])\n",
        "        data = json.loads(cleaned)  # si falla aqu√≠, la excepci√≥n sube y la capturamos arriba\n",
        "\n",
        "    # tolerancia a claves con espacios/saltos accidentales\n",
        "    if \"items\" not in data:\n",
        "        # busca una clave que contenga 'items'\n",
        "        for k in list(data.keys()):\n",
        "            if \"items\" in k.replace(\"\\n\",\"\").replace(\" \",\"\"):\n",
        "                data[\"items\"] = data.pop(k)\n",
        "                break\n",
        "\n",
        "    items = data.get(\"items\", [])\n",
        "    # normaliza ids\n",
        "    for i, it in enumerate(items, start=1):\n",
        "        it[\"id\"] = it.get(\"id\") or f\"{block_id}-Q{i}\"\n",
        "    return items\n",
        "\n",
        "def balance_and_shuffle(item, diff_threshold=4, seed=None):\n",
        "    rnd = random.Random(seed)\n",
        "    opts = item[\"options\"]\n",
        "    ci = item[\"correct_index\"]\n",
        "    Lc = wc(opts[ci])\n",
        "    new_opts = opts[:]\n",
        "    for i, opt in enumerate(new_opts):\n",
        "        if i == ci: continue\n",
        "        if (Lc - wc(opt)) > diff_threshold:\n",
        "            extra = rnd.choice([\n",
        "                \" Este patr√≥n se ha descrito en estudios de priming y decisi√≥n l√©xica.\",\n",
        "                \" La literatura lo vincula con activaci√≥n competitiva y control inhibitorio.\",\n",
        "                \" Se replica en lectores con distintos niveles de proficiencia.\"\n",
        "            ])\n",
        "            new_opts[i] = (opt.strip() + extra)\n",
        "    pairs = [(o, i==ci) for i,o in enumerate(new_opts)]\n",
        "    rnd.shuffle(pairs)\n",
        "    item[\"options\"] = [p[0] for p in pairs]\n",
        "    item[\"correct_index\"] = next(i for i,p in enumerate(pairs) if p[1])\n",
        "    return item\n",
        "\n",
        "def validate_item(it):\n",
        "    ok = True; reasons = []\n",
        "    if len(it.get(\"options\",[])) != 3:\n",
        "        ok=False; reasons.append(\"No hay 3 opciones.\")\n",
        "    if not (0 <= it.get(\"correct_index\", -1) < 3):\n",
        "        ok=False; reasons.append(\"√çndice de correcta inv√°lido.\")\n",
        "    if ok:\n",
        "        s = set([o.strip().lower() for o in it[\"options\"]])\n",
        "        if len(s) < 3:\n",
        "            ok=False; reasons.append(\"Opciones duplicadas o id√©nticas.\")\n",
        "    if wc(it.get(\"stem\",\"\")) < 6:\n",
        "        ok=False; reasons.append(\"Enunciado demasiado corto.\")\n",
        "    return ok, reasons\n",
        "\n",
        "def to_moodle_xml(items, xml_path=\"equilibrado_IA.xml\"):\n",
        "    soup = BeautifulSoup('<?xml version=\"1.0\" encoding=\"UTF-8\"?><quiz></quiz>', \"xml\")\n",
        "    quiz = soup.find(\"quiz\")\n",
        "    for it in items:\n",
        "        q = soup.new_tag(\"question\", type=\"multichoice\")\n",
        "        qt = soup.new_tag(\"questiontext\", format=\"html\")\n",
        "        qt_text = soup.new_tag(\"text\"); qt_text.string = it[\"stem\"]\n",
        "        qt.append(qt_text); q.append(qt)\n",
        "        for i,opt in enumerate(it[\"options\"]):\n",
        "            ans = soup.new_tag(\"answer\", fraction=\"100\" if i==it[\"correct_index\"] else \"0\")\n",
        "            at = soup.new_tag(\"text\"); at.string = opt\n",
        "            ans.append(at); q.append(ans)\n",
        "        quiz.append(q)\n",
        "    # salida robusta con lxml\n",
        "    xml_str = str(soup)\n",
        "    parser = etree.XMLParser(recover=True)\n",
        "    root = etree.fromstring(xml_str.encode(\"utf-8\"), parser=parser)\n",
        "    xml_bytes = etree.tostring(root, encoding=\"utf-8\", xml_declaration=True, pretty_print=True)\n",
        "    with open(xml_path, \"wb\") as f:\n",
        "        f.write(xml_bytes)\n",
        "    return xml_path\n",
        "\n",
        "def to_pdf(items, pdf_path=\"equilibrado_IA.pdf\"):\n",
        "    doc = SimpleDocTemplate(pdf_path, pagesize=A4)\n",
        "    styles = getSampleStyleSheet()\n",
        "    story = [Paragraph(\"<b>Banco de preguntas (IA)</b>\", styles[\"Title\"]), Spacer(1,10)]\n",
        "    for i,it in enumerate(items, start=1):\n",
        "        story.append(Paragraph(f\"<b>{i}. {it['stem']}</b>\", styles[\"Normal\"]))\n",
        "        for j,opt in enumerate(it[\"options\"]):\n",
        "            mark = \" ‚úÖ\" if j==it[\"correct_index\"] else \"\"\n",
        "            story.append(Paragraph(f\"{chr(97+j)}) {opt}{mark}\", styles[\"Normal\"]))\n",
        "        if it.get(\"justification\"):\n",
        "            story.append(Paragraph(f\"<i>Justificaci√≥n:</i> {it['justification']}\", styles[\"Normal\"]))\n",
        "        story.append(Spacer(1,8))\n",
        "    doc.build(story)\n",
        "    return pdf_path\n",
        "\n",
        "# ===== Generaci√≥n por bloques =====\n",
        "all_items = []\n",
        "for bi, ch in enumerate(tqdm(chunks, desc=\"Generando √≠tems IA\"), start=1):\n",
        "    try:\n",
        "        items = llm_items_from_text(ch, block_id=f\"B{bi}\", n=ITEMS_PER_BLOCK)\n",
        "        for it in items:\n",
        "            it = balance_and_shuffle(it, diff_threshold=THRESH_DIFF, seed=RANDOM_SEED)\n",
        "            ok, reasons = validate_item(it)\n",
        "            if ok:\n",
        "                all_items.append(it)\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Bloque {bi}: error de generaci√≥n ‚Üí\", e)\n",
        "        traceback.print_exc()\n",
        "\n",
        "print(f\"‚úÖ √çtems v√°lidos totales: {len(all_items)}\")\n",
        "\n",
        "# ===== Exportar =====\n",
        "xml_path = to_moodle_xml(all_items, xml_path=\"equilibrado_IA.xml\")\n",
        "pdf_path = to_pdf(all_items, pdf_path=\"equilibrado_IA.pdf\")\n",
        "print(\"üì¶ XML:\", xml_path)\n",
        "print(\"üìÑ PDF:\", pdf_path)\n",
        "\n",
        "# Descargas\n",
        "files.download(xml_path)\n",
        "files.download(pdf_path)\n"
      ],
      "id": "pipeline"
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "#  Generaci√≥n por LOTES y objetivo total (p.ej., 200 preguntas)\n",
        "#  - Pide ITEMS_PER_CALL √≠tems por llamada (p.ej., 20)\n",
        "#  - Recorre los chunks del documento en round-robin\n",
        "#  - Acumula hasta TARGET_ITEMS\n",
        "#  - Exporta un √∫nico XML+PDF con todo\n",
        "# =========================\n",
        "\n",
        "# ‚öôÔ∏è Par√°metros de lote y objetivo\n",
        "TARGET_ITEMS    = 200   # objetivo total del banco\n",
        "ITEMS_PER_CALL  = 20    # tama√±o del lote por petici√≥n a la IA\n",
        "ROUND_ROBIN     = True  # True: recorre los chunks en bucle; False: secuencial y se detiene al final\n",
        "ALLOW_DUPLICATE_STEMS = False  # si False, se evita acumular enunciados repetidos\n",
        "\n",
        "# (Opcional) reduce consumo si hace falta:\n",
        "# ITEMS_PER_CALL = 10  # menos √≠tems por llamada\n",
        "# TARGET_ITEMS   = 100 # banco m√°s peque√±o\n",
        "\n",
        "assert 'chunks' in globals() and len(chunks) > 0, \"No hay 'chunks' cargados. Sube antes tu documento y divide en chunks.\"\n",
        "assert 'llm_items_from_text' in globals(), \"Falta la funci√≥n llm_items_from_text (pega primero los helpers).\"\n",
        "\n",
        "collected = []\n",
        "seen_stems = set()\n",
        "\n",
        "def _norm_stem(s: str) -> str:\n",
        "    return re.sub(r\"\\s+\", \" \", (s or \"\").strip().lower())\n",
        "\n",
        "calls = 0\n",
        "idx = 0\n",
        "bar = tqdm(total=TARGET_ITEMS, desc=\"Acumulando √≠tems\", unit=\"itm\")\n",
        "\n",
        "try:\n",
        "    while len(collected) < TARGET_ITEMS:\n",
        "        # Elegir chunk\n",
        "        if idx >= len(chunks):\n",
        "            if ROUND_ROBIN:\n",
        "                idx = 0\n",
        "            else:\n",
        "                print(\"‚èπÔ∏è Fin de documento y objetivo no alcanzado.\")\n",
        "                break\n",
        "\n",
        "        ch = chunks[idx]\n",
        "        remaining = TARGET_ITEMS - len(collected)\n",
        "        n_this_call = min(ITEMS_PER_CALL, remaining)\n",
        "\n",
        "        try:\n",
        "            # ‚õèÔ∏è Generar lote\n",
        "            items = llm_items_from_text(ch, block_id=f\"L{calls+1}\", n=n_this_call)\n",
        "        except RuntimeError as e:\n",
        "            # p.ej., insufficient_quota (si usas mi helper con backoff)\n",
        "            print(str(e))\n",
        "            break\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Error en llamada {calls+1} (chunk {idx+1}):\", e)\n",
        "            traceback.print_exc()\n",
        "            # Pasamos al siguiente chunk/lote\n",
        "            idx += 1\n",
        "            continue\n",
        "\n",
        "        # Post-proceso y filtrado\n",
        "        added_now = 0\n",
        "        for it in items:\n",
        "            it = balance_and_shuffle(it)\n",
        "            if not validate_item(it):\n",
        "                continue\n",
        "            if not ALLOW_DUPLICATE_STEMS:\n",
        "                key = _norm_stem(it.get(\"stem\",\"\"))\n",
        "                if key in seen_stems:\n",
        "                    continue\n",
        "                seen_stems.add(key)\n",
        "\n",
        "            collected.append(it)\n",
        "            added_now += 1\n",
        "            bar.update(1)\n",
        "            if len(collected) >= TARGET_ITEMS:\n",
        "                break\n",
        "\n",
        "        # Avances\n",
        "        calls += 1\n",
        "        idx += 1\n",
        "\n",
        "        # Si el chunk rinde poco, seguimos al siguiente; con ROUND_ROBIN True volveremos a √©l en una vuelta\n",
        "\n",
        "finally:\n",
        "    bar.close()\n",
        "    print(f\"‚úÖ Total acumulado: {len(collected)} √≠tems en {calls} llamadas\")\n",
        "\n",
        "# ‚úÇÔ∏è Ajustar exactamente al objetivo (por si nos pasamos en el √∫ltimo lote)\n",
        "if len(collected) > TARGET_ITEMS:\n",
        "    collected = collected[:TARGET_ITEMS]\n",
        "\n",
        "# üóÇÔ∏è Exportar banco √∫nico\n",
        "from datetime import datetime\n",
        "stamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "xml_name = f\"banco_IA_{len(collected)}_{stamp}.xml\"\n",
        "pdf_name = f\"banco_IA_{len(collected)}_{stamp}.pdf\"\n",
        "\n",
        "xml_path = to_moodle_xml(collected, xml_path=xml_name)\n",
        "pdf_path = to_pdf(collected, pdf_path=pdf_name)\n",
        "print(\"üì¶ XML:\", xml_path)\n",
        "print(\"üìÑ PDF:\", pdf_path)\n",
        "\n",
        "# Descargas\n",
        "from google.colab import files\n",
        "files.download(xml_path)\n",
        "files.download(pdf_path)\n"
      ],
      "metadata": {
        "id": "99c2MlpjSEIQ"
      },
      "id": "99c2MlpjSEIQ",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}