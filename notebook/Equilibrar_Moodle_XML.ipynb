{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "intro"
      },
      "source": [
        "# üß† Generador IA de bancos Moodle XML + Equilibrado y PDF\n",
        "\n",
        "Este cuaderno:\n",
        "1) **Sube un documento** base (PDF/DOCX/TXT).\n",
        "2) La IA **genera preguntas MCQ** (autocontenidas, 1 correcta + 2 distractores plausibles).\n",
        "3) **Equilibra** longitudes de opciones (¬±4 palabras) y **baraja** opciones.\n",
        "4) Exporta **Moodle XML** listo para importar y un **PDF** de revisi√≥n (‚úÖ en la correcta).\n",
        "\n",
        "**Uso:** ejecuta las celdas en orden. Cuando pida *subir archivo*, selecciona tu documento fuente.\n"
      ],
      "id": "intro"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install"
      },
      "outputs": [],
      "source": [
        "!pip install openai==1.* beautifulsoup4 lxml reportlab pypdf python-docx tqdm --quiet"
      ],
      "id": "install"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "apikey"
      },
      "outputs": [],
      "source": [
        "# üîë Establece tu API key de OpenAI de forma segura (no queda guardada en el cuaderno)\n",
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "if not os.getenv(\"OPENAI_API_KEY\"):\n",
        "    os.environ[\"OPENAI_API_KEY\"] = getpass(\"Pega tu OPENAI_API_KEY y pulsa Enter: \")\n",
        "print(\"‚úÖ API key configurada en el entorno de ejecuci√≥n.\")"
      ],
      "id": "apikey"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pipeline"
      },
      "outputs": [],
      "source": [
        "# =========================\n",
        "#  Generar √≠tems con IA + equilibrar + exportar XML + PDF\n",
        "# =========================\n",
        "import os, json, random, re, traceback\n",
        "from tqdm import tqdm\n",
        "from pypdf import PdfReader\n",
        "from docx import Document\n",
        "from google.colab import files\n",
        "from bs4 import BeautifulSoup\n",
        "from bs4.element import Tag\n",
        "from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer\n",
        "from reportlab.lib.pagesizes import A4\n",
        "from reportlab.lib.styles import getSampleStyleSheet\n",
        "from lxml import etree\n",
        "\n",
        "# SDK OpenAI 1.x\n",
        "from openai import OpenAI\n",
        "\n",
        "# ===== Par√°metros =====\n",
        "THRESH_DIFF = 4            # diferencia m√°x. de palabras entre correcta e incorrectas\n",
        "RANDOM_SEED = 42           # usa None para aleatoriedad no determinista\n",
        "ITEMS_PER_BLOCK = 6        # preguntas IA por bloque de texto\n",
        "CHUNK_MAX_CHARS = 6000     # tama√±o aprox. de cada bloque del documento\n",
        "OPENAI_MODEL = \"gpt-4o-mini\"  # modelo de generaci√≥n\n",
        "DEBUG_JSON = False         # pon True si quieres ver limpieza de JSON del modelo\n",
        "\n",
        "if RANDOM_SEED is not None:\n",
        "    random.seed(RANDOM_SEED)\n",
        "\n",
        "# ===== Carga de documento base =====\n",
        "print(\"üìÅ Sube tu documento base (PDF/DOCX/TXT)\")\n",
        "up = files.upload()\n",
        "SRC = list(up.keys())[0]\n",
        "print(f\"‚úÖ Cargado: {SRC}\")\n",
        "\n",
        "def load_text(path):\n",
        "    p = path.lower()\n",
        "    if p.endswith(\".pdf\"):\n",
        "        reader = PdfReader(path)\n",
        "        return \"\\n\".join([(page.extract_text() or \"\") for page in reader.pages])\n",
        "    if p.endswith(\".docx\"):\n",
        "        doc = Document(path)\n",
        "        return \"\\n\".join([para.text for para in doc.paragraphs])\n",
        "    with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
        "        return f.read()\n",
        "\n",
        "raw_text = load_text(SRC)\n",
        "assert raw_text.strip(), \"El documento parece vac√≠o o no se pudo extraer texto.\"\n",
        "\n",
        "def chunk_text(text, max_chars=10000):\n",
        "    paras = [p.strip() for p in text.split(\"\\n\") if p.strip()]\n",
        "    chunks, cur = [], \"\"\n",
        "    for p in paras:\n",
        "        if len(cur) + len(p) + 1 <= max_chars:\n",
        "            cur += (\"\\n\" + p) if cur else p\n",
        "        else:\n",
        "            chunks.append(cur); cur = p\n",
        "    if cur: chunks.append(cur)\n",
        "    return chunks\n",
        "\n",
        "chunks = chunk_text(raw_text, max_chars=CHUNK_MAX_CHARS)\n",
        "print(f\"üß© Bloques de texto creados: {len(chunks)} (‚âà{CHUNK_MAX_CHARS} chars c/u)\")\n",
        "\n",
        "# ===== Cliente OpenAI =====\n",
        "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "if not api_key:\n",
        "    from getpass import getpass\n",
        "    api_key = getpass(\"Pega tu OPENAI_API_KEY y pulsa Enter: \")\n",
        "    os.environ[\"OPENAI_API_KEY\"] = api_key\n",
        "client = OpenAI(api_key=api_key)\n",
        "\n",
        "SYSTEM_PROMPT = (\n",
        "    \"Eres un generador de √≠tems universitarios, preciso, en espa√±ol, y devuelves JSON v√°lido.\"\n",
        ")\n",
        "\n",
        "# IMPORTANTE: llaves de JSON escapadas con {{ }}\n",
        "USER_PROMPT_TMPL = \"\"\"\n",
        "Eres un generador de preguntas universitarias en psicolog√≠a del lenguaje/lectura.\n",
        "Crea {n} preguntas tipo test (MCQ) AUTOCONTENIDAS a partir del CONTENIDO. Nivel: universitario.\n",
        "- 1 correcta + 2 distractores plausibles (sin 'todas las anteriores' / 'ninguna').\n",
        "- Redacci√≥n clara, sin ambig√ºedad; NO dependas de \"seg√∫n el texto\".\n",
        "- Incluye justificaci√≥n breve (1‚Äì2 frases) para la correcta.\n",
        "- RESPONDE √öNICAMENTE con JSON. NO incluyas explicaciones; NO uses ```json ni fences.\n",
        "\n",
        "Estructura EXACTA:\n",
        "{{\n",
        "  \"items\": [\n",
        "    {{\n",
        "      \"id\": \"BLOQUE1-Q1\",\n",
        "      \"stem\": \"ENUNCIADO AUTOCONTENIDO...\",\n",
        "      \"options\": [\"A...\", \"B...\", \"C...\"],\n",
        "      \"correct_index\": 1,\n",
        "      \"justification\": \"Por qu√© es correcta...\",\n",
        "      \"difficulty\": \"media\",\n",
        "      \"tags\": [\"efectos de priming\",\"l√©xico\"]\n",
        "    }}\n",
        "  ]\n",
        "}}\n",
        "\n",
        "CONTENIDO:\n",
        "{content}\n",
        "\"\"\"\n",
        "\n",
        "def wc(s):\n",
        "    s = re.sub(r\"\\s+\", \" \", s or \"\").strip()\n",
        "    return len([w for w in s.split(\" \") if w])\n",
        "\n",
        "# --- Helpers robustos para limpiar JSON del modelo ---\n",
        "def _strip_code_fences(s: str) -> str:\n",
        "    s = s.strip()\n",
        "    fence = re.compile(r\"^```(?:json)?\\s*([\\s\\S]*?)\\s*```$\", re.IGNORECASE)\n",
        "    m = fence.match(s)\n",
        "    return m.group(1).strip() if m else s\n",
        "\n",
        "def _extract_json_object(s: str) -> str:\n",
        "    start = s.find(\"{\")\n",
        "    if start == -1:\n",
        "        return s\n",
        "    depth = 0\n",
        "    for i, ch in enumerate(s[start:], start=start):\n",
        "        if ch == \"{\":\n",
        "            depth += 1\n",
        "        elif ch == \"}\":\n",
        "            depth -= 1\n",
        "            if depth == 0:\n",
        "                return s[start:i+1]\n",
        "    return s\n",
        "\n",
        "def llm_items_from_text(content, block_id=\"B1\", n=6, debug=DEBUG_JSON):\n",
        "    prompt = USER_PROMPT_TMPL.format(content=content, n=n)\n",
        "\n",
        "    # Intento 1: con response_format=json_object (si est√° disponible para tu cuenta)\n",
        "    try:\n",
        "        resp = client.chat.completions.create(\n",
        "            model=OPENAI_MODEL,\n",
        "            messages=[\n",
        "                {\"role\":\"system\",\"content\":SYSTEM_PROMPT},\n",
        "                {\"role\":\"user\",\"content\":prompt}\n",
        "            ],\n",
        "            temperature=0.4,\n",
        "            response_format={\"type\":\"json_object\"}\n",
        "        )\n",
        "        raw = resp.choices[0].message.content\n",
        "    except Exception:\n",
        "        # Intento 2: sin response_format\n",
        "        resp = client.chat.completions.create(\n",
        "            model=OPENAI_MODEL,\n",
        "            messages=[\n",
        "                {\"role\":\"system\",\"content\":SYSTEM_PROMPT},\n",
        "                {\"role\":\"user\",\"content\":prompt}\n",
        "            ],\n",
        "            temperature=0.4\n",
        "        )\n",
        "        raw = resp.choices[0].message.content\n",
        "\n",
        "    # Parseo tolerante\n",
        "    try:\n",
        "        data = json.loads(raw)\n",
        "    except Exception:\n",
        "        cleaned = _strip_code_fences(raw)\n",
        "        cleaned = _extract_json_object(cleaned)\n",
        "        if debug:\n",
        "            print(\"DEBUG raw[:400]:\", raw[:400])\n",
        "            print(\"DEBUG cleaned[:400]:\", cleaned[:400])\n",
        "        data = json.loads(cleaned)\n",
        "\n",
        "    # Tolerancia a claves raras tipo '\\n  \"items\"'\n",
        "    if \"items\" not in data:\n",
        "        for k in list(data.keys()):\n",
        "            if \"items\" in k.replace(\"\\n\",\"\").replace(\" \",\"\"):\n",
        "                data[\"items\"] = data.pop(k)\n",
        "                break\n",
        "\n",
        "    items = data.get(\"items\", [])\n",
        "    # Normaliza ids y estructura\n",
        "    norm = []\n",
        "    for i, it in enumerate(items, start=1):\n",
        "        stem = it.get(\"stem\",\"\").strip()\n",
        "        options = it.get(\"options\", [])\n",
        "        ci = it.get(\"correct_index\", 0)\n",
        "        if len(options) != 3:\n",
        "            continue\n",
        "        norm.append({\n",
        "            \"id\": it.get(\"id\") or f\"{block_id}-Q{i}\",\n",
        "            \"stem\": stem,\n",
        "            \"options\": [str(o).strip() for o in options],\n",
        "            \"correct_index\": int(ci),\n",
        "            \"justification\": it.get(\"justification\",\"\").strip(),\n",
        "            \"difficulty\": it.get(\"difficulty\",\"media\"),\n",
        "            \"tags\": it.get(\"tags\",[])\n",
        "        })\n",
        "    return norm\n",
        "\n",
        "def balance_and_shuffle(item, diff_threshold=4, seed=None):\n",
        "    rnd = random.Random(seed)\n",
        "    opts = item[\"options\"]\n",
        "    ci = item[\"correct_index\"]\n",
        "    Lc = wc(opts[ci])\n",
        "    new_opts = opts[:]\n",
        "    for i, opt in enumerate(new_opts):\n",
        "        if i == ci:\n",
        "            continue\n",
        "        if (Lc - wc(opt)) > diff_threshold:\n",
        "            extra = rnd.choice([\n",
        "                \" Este patr√≥n se ha descrito en estudios de priming y decisi√≥n l√©xica.\",\n",
        "                \" La literatura lo vincula con activaci√≥n competitiva y control inhibitorio.\",\n",
        "                \" Se replica en lectores con distintos niveles de proficiencia.\"\n",
        "            ])\n",
        "            new_opts[i] = (opt.strip() + extra)\n",
        "    pairs = [(o, i==ci) for i,o in enumerate(new_opts)]\n",
        "    rnd.shuffle(pairs)\n",
        "    item[\"options\"] = [p[0] for p in pairs]\n",
        "    item[\"correct_index\"] = next(i for i,p in enumerate(pairs) if p[1])\n",
        "    return item\n",
        "\n",
        "def validate_item(it):\n",
        "    ok = True; reasons = []\n",
        "    if len(it.get(\"options\",[])) != 3:\n",
        "        ok=False; reasons.append(\"No hay 3 opciones.\")\n",
        "    if not (0 <= it.get(\"correct_index\", -1) < 3):\n",
        "        ok=False; reasons.append(\"√çndice de correcta inv√°lido.\")\n",
        "    if ok:\n",
        "        s = set([o.strip().lower() for o in it[\"options\"]])\n",
        "        if len(s) < 3:\n",
        "            ok=False; reasons.append(\"Opciones duplicadas o id√©nticas.\")\n",
        "    if wc(it.get(\"stem\",\"\")) < 6:\n",
        "        ok=False; reasons.append(\"Enunciado demasiado corto.\")\n",
        "    return ok, reasons\n",
        "\n",
        "def to_moodle_xml(items, xml_path=\"equilibrado_IA.xml\"):\n",
        "    soup = BeautifulSoup('<?xml version=\"1.0\" encoding=\"UTF-8\"?><quiz></quiz>', \"xml\")\n",
        "    quiz = soup.find(\"quiz\")\n",
        "    for it in items:\n",
        "        q = soup.new_tag(\"question\", type=\"multichoice\")\n",
        "        qt = soup.new_tag(\"questiontext\", format=\"html\")\n",
        "        qt_text = soup.new_tag(\"text\"); qt_text.string = it[\"stem\"]\n",
        "        qt.append(qt_text); q.append(qt)\n",
        "        for i,opt in enumerate(it[\"options\"]):\n",
        "            ans = soup.new_tag(\"answer\", fraction=\"100\" if i==it[\"correct_index\"] else \"0\")\n",
        "            at = soup.new_tag(\"text\"); at.string = opt\n",
        "            ans.append(at); q.append(ans)\n",
        "        quiz.append(q)\n",
        "    # salida robusta con lxml\n",
        "    xml_str = str(soup)\n",
        "    parser = etree.XMLParser(recover=True)\n",
        "    root = etree.fromstring(xml_str.encode(\"utf-8\"), parser=parser)\n",
        "    xml_bytes = etree.tostring(root, encoding=\"utf-8\", xml_declaration=True, pretty_print=True)\n",
        "    with open(xml_path, \"wb\") as f:\n",
        "        f.write(xml_bytes)\n",
        "    return xml_path\n",
        "\n",
        "def to_pdf(items, pdf_path=\"equilibrado_IA.pdf\"):\n",
        "    doc = SimpleDocTemplate(pdf_path, pagesize=A4)\n",
        "    styles = getSampleStyleSheet()\n",
        "    story = [Paragraph(\"<b>Banco de preguntas (IA)</b>\", styles[\"Title\"]), Spacer(1,10)]\n",
        "    for i,it in enumerate(items, start=1):\n",
        "        story.append(Paragraph(f\"<b>{i}. {it['stem']}</b>\", styles[\"Normal\"]))\n",
        "        for j,opt in enumerate(it[\"options\"]):\n",
        "            mark = \" ‚úÖ\" if j==it[\"correct_index\"] else \"\"\n",
        "            story.append(Paragraph(f\"{chr(97+j)}) {opt}{mark}\", styles[\"Normal\"]))\n",
        "        if it.get(\"justification\"):\n",
        "            story.append(Paragraph(f\"<i>Justificaci√≥n:</i> {it['justification']}\", styles[\"Normal\"]))\n",
        "        story.append(Spacer(1,8))\n",
        "    doc.build(story)\n",
        "    return pdf_path\n",
        "\n",
        "# ===== Generaci√≥n por bloques =====\n",
        "all_items = []\n",
        "for bi, ch in enumerate(tqdm(chunks, desc=\"Generando √≠tems IA\"), start=1):\n",
        "    try:\n",
        "        items = llm_items_from_text(ch, block_id=f\"B{bi}\", n=ITEMS_PER_BLOCK)\n",
        "        for it in items:\n",
        "            it = balance_and_shuffle(it, diff_threshold=THRESH_DIFF, seed=RANDOM_SEED)\n",
        "            ok, reasons = validate_item(it)\n",
        "            if ok:\n",
        "                all_items.append(it)\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Bloque {bi}: error de generaci√≥n ‚Üí\", e)\n",
        "        traceback.print_exc()\n",
        "\n",
        "print(f\"‚úÖ √çtems v√°lidos totales: {len(all_items)}\")\n",
        "\n",
        "# ===== Exportar =====\n",
        "xml_path = to_moodle_xml(all_items, xml_path=\"equilibrado_IA.xml\")\n",
        "pdf_path = to_pdf(all_items, pdf_path=\"equilibrado_IA.pdf\")\n",
        "print(\"üì¶ XML:\", xml_path)\n",
        "print(\"üìÑ PDF:\", pdf_path)\n",
        "\n",
        "# Descargas\n",
        "files.download(xml_path)\n",
        "files.download(pdf_path)\n",
        "\n"
      ],
      "id": "pipeline"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}