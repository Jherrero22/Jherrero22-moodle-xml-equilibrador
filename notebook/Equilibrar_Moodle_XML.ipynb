{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "intro"
      },
      "source": [
        "# üß† Generador IA de bancos Moodle XML + Equilibrado y PDF\n",
        "\n",
        "Este cuaderno:\n",
        "1) **Sube un documento** base (PDF/DOCX/TXT).\n",
        "2) La IA **genera preguntas MCQ** (autocontenidas, 1 correcta + 2 distractores plausibles).\n",
        "3) **Equilibra** longitudes de opciones (¬±4 palabras) y **baraja** opciones.\n",
        "4) Exporta **Moodle XML** listo para importar y un **PDF** de revisi√≥n (‚úÖ en la correcta).\n",
        "\n",
        "**Uso:** ejecuta las celdas en orden. Cuando pida *subir archivo*, selecciona tu documento fuente.\n"
      ],
      "id": "intro"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install"
      },
      "outputs": [],
      "source": [
        "!pip install openai==1.* beautifulsoup4 lxml reportlab pypdf python-docx tqdm --quiet"
      ],
      "id": "install"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "apikey",
        "outputId": "beb08588-159d-4658-8dbe-6dbb6d12b26a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ API key configurada en el entorno de ejecuci√≥n.\n"
          ]
        }
      ],
      "source": [
        "# üîë Establece tu API key de OpenAI de forma segura (no queda guardada en el cuaderno)\n",
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "if not os.getenv(\"OPENAI_API_KEY\"):\n",
        "    os.environ[\"OPENAI_API_KEY\"] = getpass(\"Pega tu OPENAI_API_KEY y pulsa Enter: \")\n",
        "print(\"‚úÖ API key configurada en el entorno de ejecuci√≥n.\")"
      ],
      "id": "apikey"
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "#  N√∫cleo: carga de documento, helpers IA, equilibrado, exportadores\n",
        "# =========================\n",
        "import os, json, random, re, time, traceback\n",
        "from tqdm import tqdm\n",
        "from pypdf import PdfReader\n",
        "from docx import Document\n",
        "from google.colab import files\n",
        "from bs4 import BeautifulSoup\n",
        "from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer\n",
        "from reportlab.lib.pagesizes import A4\n",
        "from reportlab.lib.styles import getSampleStyleSheet\n",
        "from lxml import etree\n",
        "\n",
        "# SDK OpenAI 1.x\n",
        "from openai import OpenAI, RateLimitError\n",
        "\n",
        "# ===== Par√°metros generales =====\n",
        "THRESH_DIFF = 4            # diferencia m√°x. de palabras entre correcta e incorrectas\n",
        "RANDOM_SEED = 42           # usa None para aleatoriedad no determinista\n",
        "CHUNK_MAX_CHARS = 6000     # tama√±o aprox. de cada bloque del documento\n",
        "OPENAI_MODEL = \"gpt-4o-mini\"\n",
        "DEBUG_JSON = False\n",
        "\n",
        "# Reintentos/backoff para llamadas a la IA\n",
        "MAX_RETRIES = 4\n",
        "BACKOFF_BASE = 2.0\n",
        "\n",
        "if RANDOM_SEED is not None:\n",
        "    random.seed(RANDOM_SEED)\n",
        "\n",
        "# ===== Carga de documento base =====\n",
        "print(\"üìÅ Sube tu documento base (PDF/DOCX/TXT)\")\n",
        "up = files.upload()\n",
        "SRC = list(up.keys())[0]\n",
        "print(f\"‚úÖ Cargado: {SRC}\")\n",
        "\n",
        "def load_text(path):\n",
        "    p = path.lower()\n",
        "    if p.endswith(\".pdf\"):\n",
        "        reader = PdfReader(path)\n",
        "        return \"\\n\".join([(page.extract_text() or \"\") for page in reader.pages])\n",
        "    if p.endswith(\".docx\"):\n",
        "        doc = Document(path)\n",
        "        return \"\\n\".join([para.text for para in doc.paragraphs])\n",
        "    with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
        "        return f.read()\n",
        "\n",
        "raw_text = load_text(SRC)\n",
        "assert raw_text.strip(), \"El documento parece vac√≠o o no se pudo extraer texto.\"\n",
        "\n",
        "def chunk_text(text, max_chars=10000):\n",
        "    paras = [p.strip() for p in text.split(\"\\n\") if p.strip()]\n",
        "    chunks, cur = [], \"\"\n",
        "    for p in paras:\n",
        "        if len(cur) + len(p) + 1 <= max_chars:\n",
        "            cur += (\"\\n\" + p) if cur else p\n",
        "        else:\n",
        "            chunks.append(cur); cur = p\n",
        "    if cur: chunks.append(cur)\n",
        "    return chunks\n",
        "\n",
        "chunks = chunk_text(raw_text, max_chars=CHUNK_MAX_CHARS)\n",
        "print(f\"üß© Bloques de texto creados: {len(chunks)} (‚âà{CHUNK_MAX_CHARS} chars c/u)\")\n",
        "\n",
        "# ===== Cliente OpenAI =====\n",
        "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "if not api_key:\n",
        "    from getpass import getpass\n",
        "    api_key = getpass(\"Pega tu OPENAI_API_KEY y pulsa Enter: \")\n",
        "    os.environ[\"OPENAI_API_KEY\"] = api_key\n",
        "client = OpenAI(api_key=api_key)\n",
        "\n",
        "SYSTEM_PROMPT = \"Eres un generador de √≠tems universitarios, preciso, en espa√±ol, y devuelves JSON v√°lido.\"\n",
        "\n",
        "# IMPORTANTE: llaves JSON escapadas con {{ }} para no romper str.format\n",
        "USER_PROMPT_TMPL = \"\"\"\n",
        "Eres un generador de preguntas universitarias en psicolog√≠a del lenguaje/lectura.\n",
        "Crea {n} preguntas tipo test (MCQ) AUTOCONTENIDAS a partir del CONTENIDO. Nivel: universitario.\n",
        "- 1 correcta + 2 distractores plausibles (sin 'todas las anteriores' / 'ninguna').\n",
        "- Redacci√≥n clara, sin ambig√ºedad; NO dependas de \"seg√∫n el texto\".\n",
        "- Incluye justificaci√≥n breve (1‚Äì2 frases) para la correcta.\n",
        "- RESPONDE √öNICAMENTE con JSON. NO incluyas explicaciones; NO uses ```json ni fences.\n",
        "\n",
        "Estructura EXACTA:\n",
        "{{\n",
        "  \"items\": [\n",
        "    {{\n",
        "      \"id\": \"BLOQUE1-Q1\",\n",
        "      \"stem\": \"ENUNCIADO AUTOCONTENIDO...\",\n",
        "      \"options\": [\"A...\", \"B...\", \"C...\"],\n",
        "      \"correct_index\": 1,\n",
        "      \"justification\": \"Por qu√© es correcta...\",\n",
        "      \"difficulty\": \"media\",\n",
        "      \"tags\": [\"efectos de priming\",\"l√©xico\"]\n",
        "    }}\n",
        "  ]\n",
        "}}\n",
        "\n",
        "CONTENIDO:\n",
        "{content}\n",
        "\"\"\"\n",
        "\n",
        "def wc(s):\n",
        "    s = re.sub(r\"\\s+\", \" \", s or \"\").strip()\n",
        "    return len([w for w in s.split(\" \") if w])\n",
        "\n",
        "# --- Limpieza de JSON devuelto por el modelo ---\n",
        "def _strip_code_fences(s: str) -> str:\n",
        "    s = s.strip()\n",
        "    m = re.match(r\"^```(?:json)?\\s*([\\s\\S]*?)\\s*```$\", s, flags=re.I)\n",
        "    return m.group(1).strip() if m else s\n",
        "\n",
        "def _extract_json_object(s: str) -> str:\n",
        "    start = s.find(\"{\")\n",
        "    if start == -1: return s\n",
        "    depth = 0\n",
        "    for i, ch in enumerate(s[start:], start=start):\n",
        "        if ch == \"{\": depth += 1\n",
        "        elif ch == \"}\":\n",
        "            depth -= 1\n",
        "            if depth == 0: return s[start:i+1]\n",
        "    return s\n",
        "\n",
        "def _request_openai(prompt: str, retries=MAX_RETRIES):\n",
        "    \"\"\"Llamada con reintentos/backoff; aborta limpio si falta cr√©dito.\"\"\"\n",
        "    attempt = 0\n",
        "    while True:\n",
        "        try:\n",
        "            # Intento 1: con response_format=json_object\n",
        "            return client.chat.completions.create(\n",
        "                model=OPENAI_MODEL,\n",
        "                messages=[{\"role\":\"system\",\"content\":SYSTEM_PROMPT},\n",
        "                          {\"role\":\"user\",\"content\":prompt}],\n",
        "                temperature=0.4,\n",
        "                response_format={\"type\":\"json_object\"}\n",
        "            )\n",
        "        except RateLimitError as e:\n",
        "            msg = str(e)\n",
        "            if \"insufficient_quota\" in msg:\n",
        "                raise RuntimeError(\"‚ùå Sin cr√©dito en la API (insufficient_quota). Revisa plan y billing.\") from e\n",
        "            if attempt >= retries:\n",
        "                raise\n",
        "            sleep_s = BACKOFF_BASE ** attempt\n",
        "            print(f\"‚è≥ Rate limit. Reintentando en {sleep_s:.1f}s (intento {attempt+1}/{retries})‚Ä¶\")\n",
        "            time.sleep(sleep_s); attempt += 1\n",
        "        except Exception as e:\n",
        "            # Intento 2: sin response_format\n",
        "            try:\n",
        "                return client.chat.completions.create(\n",
        "                    model=OPENAI_MODEL,\n",
        "                    messages=[{\"role\":\"system\",\"content\":SYSTEM_PROMPT},\n",
        "                              {\"role\":\"user\",\"content\":prompt}],\n",
        "                    temperature=0.4\n",
        "                )\n",
        "            except Exception as e2:\n",
        "                msg = str(e2)\n",
        "                if \"insufficient_quota\" in msg:\n",
        "                    raise RuntimeError(\"‚ùå Sin cr√©dito en la API (insufficient_quota). Revisa plan y billing.\") from e2\n",
        "                if attempt >= retries:\n",
        "                    raise\n",
        "                sleep_s = BACKOFF_BASE ** attempt\n",
        "                print(f\"‚è≥ Reintentando en {sleep_s:.1f}s (intento {attempt+1}/{retries})‚Ä¶\")\n",
        "                time.sleep(sleep_s); attempt += 1\n",
        "\n",
        "def llm_items_from_text(content, block_id=\"B1\", n=6, debug=DEBUG_JSON):\n",
        "    prompt = USER_PROMPT_TMPL.format(content=content, n=n)\n",
        "    resp = _request_openai(prompt)\n",
        "    raw = resp.choices[0].message.content\n",
        "\n",
        "    # Parseo tolerante\n",
        "    try:\n",
        "        data = json.loads(raw)\n",
        "    except Exception:\n",
        "        cleaned = _strip_code_fences(raw)\n",
        "        cleaned = _extract_json_object(cleaned)\n",
        "        if debug:\n",
        "            print(\"DEBUG raw[:400]:\", raw[:400])\n",
        "            print(\"DEBUG cleaned[:400]:\", cleaned[:400])\n",
        "        data = json.loads(cleaned)\n",
        "\n",
        "    # Tolerancia a claves con saltos/espacios raros\n",
        "    if \"items\" not in data:\n",
        "        for k in list(data.keys()):\n",
        "            if \"items\" in k.replace(\"\\n\",\"\").replace(\" \",\"\"):\n",
        "                data[\"items\"] = data.pop(k)\n",
        "                break\n",
        "\n",
        "    items = data.get(\"items\", [])\n",
        "    norm = []\n",
        "    for i, it in enumerate(items, start=1):\n",
        "        opts = it.get(\"options\", [])\n",
        "        if len(opts) != 3:\n",
        "            continue\n",
        "        ci = int(it.get(\"correct_index\", 0))\n",
        "        norm.append({\n",
        "            \"id\": it.get(\"id\") or f\"{block_id}-Q{i}\",\n",
        "            \"stem\": str(it.get(\"stem\",\"\")).strip(),\n",
        "            \"options\": [str(o).strip() for o in opts],\n",
        "            \"correct_index\": ci if 0 <= ci < 3 else 0,\n",
        "            \"justification\": str(it.get(\"justification\",\"\")).strip(),\n",
        "            \"difficulty\": it.get(\"difficulty\",\"media\"),\n",
        "            \"tags\": it.get(\"tags\",[])\n",
        "        })\n",
        "    return norm\n",
        "\n",
        "def balance_and_shuffle(item, diff_threshold=THRESH_DIFF, seed=RANDOM_SEED):\n",
        "    rnd = random.Random(seed)\n",
        "    opts = item[\"options\"]; ci = item[\"correct_index\"]\n",
        "    Lc = wc(opts[ci]); new_opts = opts[:]\n",
        "    for i, opt in enumerate(new_opts):\n",
        "        if i == ci:\n",
        "            continue\n",
        "        if (Lc - wc(opt)) > diff_threshold:\n",
        "            extra = rnd.choice([\n",
        "                \" Este patr√≥n se ha descrito en estudios de priming y decisi√≥n l√©xica.\",\n",
        "                \" La literatura lo vincula con activaci√≥n competitiva y control inhibitorio.\",\n",
        "                \" Se replica en lectores con distintos niveles de proficiencia.\"\n",
        "            ])\n",
        "            new_opts[i] = (opt.strip() + extra)\n",
        "    pairs = [(o, i==ci) for i,o in enumerate(new_opts)]\n",
        "    rnd.shuffle(pairs)\n",
        "    item[\"options\"] = [p[0] for p in pairs]\n",
        "    item[\"correct_index\"] = next(i for i,p in enumerate(pairs) if p[1])\n",
        "    return item\n",
        "\n",
        "def validate_item(it):\n",
        "    ok = True\n",
        "    if len(it.get(\"options\",[])) != 3: ok=False\n",
        "    if not (0 <= it.get(\"correct_index\", -1) < 3): ok=False\n",
        "    if ok:\n",
        "        s = set([o.strip().lower() for o in it[\"options\"]])\n",
        "        if len(s) < 3: ok=False\n",
        "    if wc(it.get(\"stem\",\"\")) < 6: ok=False\n",
        "    return ok\n",
        "\n",
        "def to_moodle_xml(items, xml_path=\"equilibrado_IA.xml\"):\n",
        "    soup = BeautifulSoup('<?xml version=\"1.0\" encoding=\"UTF-8\"?><quiz></quiz>', \"xml\")\n",
        "    quiz = soup.find(\"quiz\")\n",
        "    for it in items:\n",
        "        q = soup.new_tag(\"question\", type=\"multichoice\")\n",
        "        qt = soup.new_tag(\"questiontext\", format=\"html\")\n",
        "        qt_text = soup.new_tag(\"text\"); qt_text.string = it[\"stem\"]\n",
        "        qt.append(qt_text); q.append(qt)\n",
        "        for i,opt in enumerate(it[\"options\"]):\n",
        "            ans = soup.new_tag(\"answer\", fraction=\"100\" if i==it[\"correct_index\"] else \"0\")\n",
        "            at = soup.new_tag(\"text\"); at.string = opt\n",
        "            ans.append(at); q.append(ans)\n",
        "        quiz.append(q)\n",
        "    xml_str = str(soup)\n",
        "    root = etree.fromstring(xml_str.encode(\"utf-8\"), parser=etree.XMLParser(recover=True))\n",
        "    with open(xml_path, \"wb\") as f:\n",
        "        f.write(etree.tostring(root, encoding=\"utf-8\", xml_declaration=True, pretty_print=True))\n",
        "    return xml_path\n",
        "\n",
        "def to_pdf(items, pdf_path=\"equilibrado_IA.pdf\"):\n",
        "    doc = SimpleDocTemplate(pdf_path, pagesize=A4)\n",
        "    styles = getSampleStyleSheet()\n",
        "    story = [Paragraph(\"<b>Banco de preguntas (IA)</b>\", styles[\"Title\"]), Spacer(1,10)]\n",
        "    for i,it in enumerate(items, start=1):\n",
        "        story.append(Paragraph(f\"<b>{i}. {it['stem']}</b>\", styles[\"Normal\"]))\n",
        "        for j,opt in enumerate(it[\"options\"]):\n",
        "            mark = \" ‚úÖ\" if j==it[\"correct_index\"] else \"\"\n",
        "            story.append(Paragraph(f\"{chr(97+j)}) {opt}{mark}\", styles[\"Normal\"]))\n",
        "        if it.get(\"justification\"):\n",
        "            story.append(Paragraph(f\"<i>Justificaci√≥n:</i> {it['justification']}\", styles[\"Normal\"]))\n",
        "        story.append(Spacer(1,8))\n",
        "    doc.build(story)\n",
        "    return pdf_path\n"
      ],
      "metadata": {
        "id": "EHR9_TlXWd2w",
        "outputId": "a63b0ea3-2cb1-4de0-d27c-38f14ae1f642",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "id": "EHR9_TlXWd2w",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìÅ Sube tu documento base (PDF/DOCX/TXT)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-a4ae03f9-f10b-4719-93fb-2a94a288dd63\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-a4ae03f9-f10b-4719-93fb-2a94a288dd63\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Neurociencia-del-lenguaje.pdf to Neurociencia-del-lenguaje (11).pdf\n",
            "‚úÖ Cargado: Neurociencia-del-lenguaje (11).pdf\n",
            "üß© Bloques de texto creados: 14 (‚âà6000 chars c/u)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "#  Celda 4 PRO (compatible con Colab, sin versiones fijas)\n",
        "#  - Instala paquetes si faltan\n",
        "#  - Descarga el modelo de spaCy espa√±ol (md o sm) autom√°ticamente\n",
        "#  - Fallback si no hay YAKE o modelo spaCy\n",
        "#  - Genera items mejores sin usar API y exporta XML+PDF\n",
        "# =========================\n",
        "import sys, subprocess, pkgutil, os, re, random\n",
        "from collections import Counter\n",
        "from datetime import datetime\n",
        "\n",
        "def ensure(pkg):\n",
        "    if pkg in {m.name for m in pkgutil.iter_modules()}:\n",
        "        return\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", pkg])\n",
        "\n",
        "# 1) Asegurar dependencias\n",
        "ensure(\"spacy\")\n",
        "ensure(\"scikit-learn\")\n",
        "try:\n",
        "    ensure(\"yake\")\n",
        "    import yake\n",
        "    YAKE_OK = True\n",
        "except Exception:\n",
        "    YAKE_OK = False\n",
        "    yake = None\n",
        "\n",
        "import spacy\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# 2) Cargar modelo de spaCy (md -> sm -> multiling√ºe) sin fijar versiones\n",
        "def load_es_model():\n",
        "    for model in [\"es_core_news_md\", \"es_core_news_sm\", \"xx_sent_ud_sm\"]:\n",
        "        try:\n",
        "            return spacy.load(model), model\n",
        "        except OSError:\n",
        "            try:\n",
        "                import spacy.cli\n",
        "                spacy.cli.download(model)\n",
        "                return spacy.load(model), model\n",
        "            except Exception:\n",
        "                continue\n",
        "    return None, None\n",
        "\n",
        "nlp, model_name = load_es_model()\n",
        "if nlp is None:\n",
        "    raise RuntimeError(\"No pude cargar ning√∫n modelo de spaCy. ¬øBloqueado internet del runtime?\")\n",
        "\n",
        "random.seed(42)\n",
        "\n",
        "# ---- Texto base viene de la Celda 3 ----\n",
        "def clean_spaces(s): return re.sub(r\"\\s+\", \" \", s or \"\").strip()\n",
        "doc_text = clean_spaces(raw_text)\n",
        "\n",
        "# ---- Segmentaci√≥n en oraciones con spaCy (fallback por regex si hiciera falta) ----\n",
        "def split_sentences(text, min_words=10, max_words=55):\n",
        "    sents = []\n",
        "    try:\n",
        "        doc = nlp(text)\n",
        "        for s in doc.sents:\n",
        "            t = clean_spaces(s.text)\n",
        "            wc = len(t.split())\n",
        "            if min_words <= wc <= max_words:\n",
        "                sents.append(t)\n",
        "    except Exception:\n",
        "        pass\n",
        "    if len(sents) < 40:\n",
        "        sents_alt = [clean_spaces(x) for x in re.split(r\"[\\.!?]\\s+\", text) if len(x.split()) >= min_words]\n",
        "        sents = list(dict.fromkeys(sents + sents_alt))\n",
        "    return sents\n",
        "\n",
        "sents = split_sentences(doc_text)\n",
        "assert sents, \"No se encontraron oraciones adecuadas. Baja min_words o usa otro documento.\"\n",
        "\n",
        "# ---- Palabras clave globales ----\n",
        "STOP = set(\"\"\"\n",
        "de la que el los un una y o en a para por con sin sobre entre como m√°s menos muy\n",
        "este esta estos estas ese esa esos esas aquel aquella aquellos aquellas lo al del\n",
        "\"\"\".split())\n",
        "\n",
        "def extract_global_kws(text, top=60):\n",
        "    if YAKE_OK:\n",
        "        try:\n",
        "            kw_extractor = yake.KeywordExtractor(lan=\"es\", n=1, top=top)\n",
        "            kws = [k for k,_ in kw_extractor.extract_keywords(text)]\n",
        "            return [k for k in kws if 3 <= len(k) <= 30]\n",
        "        except Exception:\n",
        "            pass\n",
        "    # Fallback TF-IDF de todo el documento\n",
        "    toks = re.findall(r\"[A-Za-z√Å√â√ç√ì√ö√ú√ë√°√©√≠√≥√∫√º√±]+\", text.lower())\n",
        "    toks = [t for t in toks if t not in STOP and len(t) > 2]\n",
        "    cnt = Counter(toks)\n",
        "    return [w for w,_ in cnt.most_common(top)]\n",
        "\n",
        "global_kws = extract_global_kws(doc_text, top=60)\n",
        "\n",
        "def wc(s): return len([w for w in re.sub(r\"\\s+\",\" \",s).split() if w])\n",
        "\n",
        "def pad_balance(base_text, target_len):\n",
        "    paddings = [\n",
        "        \" Este matiz se ha observado en estudios revisados por pares.\",\n",
        "        \" Se describe en la literatura especializada con resultados consistentes.\",\n",
        "        \" El fen√≥meno puede modularse por la tarea y el nivel de pericia.\",\n",
        "        \" El patr√≥n se replica en contextos metodol√≥gicos diversos.\"\n",
        "    ]\n",
        "    t = base_text.strip()\n",
        "    while wc(t) < target_len:\n",
        "        t += random.choice(paddings)\n",
        "        if wc(t) >= target_len: break\n",
        "    return t\n",
        "\n",
        "# ---- Distractores y plantillas ----\n",
        "ANTONYM_MAP = [\n",
        "    (\"aumenta\",\"disminuye\"), (\"incrementa\",\"reduce\"),\n",
        "    (\"facilita\",\"dificulta\"), (\"mejora\",\"empeora\"),\n",
        "    (\"r√°pido\",\"lento\"), (\"eficiente\",\"ineficiente\"),\n",
        "    (\"preciso\",\"impreciso\"), (\"automatizado\",\"manual\")\n",
        "]\n",
        "DOMAIN_HINTS = [\n",
        "    \"en tareas de decisi√≥n l√©xica\",\n",
        "    \"seg√∫n la evidencia de potenciales evocados\",\n",
        "    \"en paradigmas de priming sem√°ntico\",\n",
        "    \"en lectores con distinto nivel de proficiencia\",\n",
        "    \"seg√∫n meta-an√°lisis recientes\",\n",
        "]\n",
        "\n",
        "def flip_polarity(txt):\n",
        "    t = txt\n",
        "    for a,b in ANTONYM_MAP:\n",
        "        t = re.sub(rf\"\\b{a}\\b\", b, t, flags=re.I)\n",
        "        t = re.sub(rf\"\\b{b}\\b\", a, t, flags=re.I)\n",
        "    return t\n",
        "\n",
        "def mk_distractors_from_sentence(sent, kws):\n",
        "    base = sent\n",
        "    d1 = flip_polarity(base)\n",
        "    if d1 == base:\n",
        "        d1 = \"Generaliza el enunciado omitiendo condiciones y l√≠mites del contexto descrito.\"\n",
        "    d1 += \" \" + random.choice(DOMAIN_HINTS)\n",
        "    if len(kws) >= 2:\n",
        "        d2 = f\"Plantea causalidad directa entre ¬´{kws[0]}¬ª y ¬´{kws[1]}¬ª, aunque el texto sugiere asociaci√≥n parcial.\"\n",
        "    else:\n",
        "        d2 = \"Asume una relaci√≥n causal donde el texto solo indica covariaci√≥n limitada.\"\n",
        "    d2 += \" \" + random.choice([h for h in DOMAIN_HINTS if h not in d1] or DOMAIN_HINTS)\n",
        "    return d1.strip(), d2.strip()\n",
        "\n",
        "def item_definicion(span_text, span_head):\n",
        "    stem = f\"¬øCu√°l opci√≥n define con mayor precisi√≥n el concepto ¬´{span_head}¬ª?\"\n",
        "    correct = f\"{span_head.capitalize()}: {span_text}\"\n",
        "    d1 = f\"{span_head.capitalize()}: confunde el fen√≥meno con un correlato metodol√≥gico.\"\n",
        "    d2 = f\"{span_head.capitalize()}: exagera la generalidad sin apoyo emp√≠rico.\"\n",
        "    return stem, correct, d1, d2\n",
        "\n",
        "def item_cloze(sentence, key):\n",
        "    masked = re.sub(rf\"\\b{re.escape(key)}\\b\", \"_____\", sentence, flags=re.I)\n",
        "    if masked == sentence:\n",
        "        # si no se encontr√≥, usa una global\n",
        "        key2 = next((k for k in global_kws if k != key), \"t√©rmino\")\n",
        "        masked = re.sub(rf\"\\b{re.escape(key2)}\\b\", \"_____\", sentence, flags=re.I)\n",
        "        key = key2\n",
        "    stem = f\"Complete el enunciado con el t√©rmino que mejor mantiene su sentido: {masked}\"\n",
        "    correct = key\n",
        "    pool = [k for k in global_kws if k != key]\n",
        "    d1 = random.choice(pool or [\"concepto\"])\n",
        "    d2 = random.choice([k for k in pool if k != d1] or [\"procedimiento\"])\n",
        "    return stem, correct, d1, d2\n",
        "\n",
        "def item_excepcion(sentence, topic):\n",
        "    stem = f\"¬øCu√°l opci√≥n NO es coherente con el alcance del siguiente enunciado?: {sentence}\"\n",
        "    correct = \"Introduce una excepci√≥n que el texto no contempla, alterando el alcance interpretativo.\"\n",
        "    d1 = \"Mantiene el significado y los l√≠mites propuestos en el enunciado original.\"\n",
        "    d2 = f\"Reformula el contenido manteniendo condiciones sobre {topic}.\"\n",
        "    return stem, correct, d1, d2\n",
        "\n",
        "def item_causal(sentence, kws):\n",
        "    stem = \"¬øCu√°l opci√≥n formula adecuadamente la relaci√≥n entre los elementos descritos?\"\n",
        "    if len(kws) >= 2:\n",
        "        a,b = kws[0],kws[1]\n",
        "        correct = f\"Asocia ¬´{a}¬ª y ¬´{b}¬ª de modo condicionado, sin asumir causalidad fuerte.\"\n",
        "        d1 = f\"Afirma causalidad directa entre ¬´{a}¬ª y ¬´{b}¬ª sin evidencia.\"\n",
        "        d2 = f\"Niega toda relaci√≥n entre ¬´{a}¬ª y ¬´{b}¬ª ignorando coincidencias observadas.\"\n",
        "    else:\n",
        "        correct = \"Formula una relaci√≥n condicional moderada, acorde con la evidencia.\"\n",
        "        d1 = \"Atribuye causalidad absoluta sin soporte emp√≠rico.\"\n",
        "        d2 = \"Descarta toda relaci√≥n pese a coincidencias reportadas.\"\n",
        "    return stem, correct, d1, d2\n",
        "\n",
        "def extract_head_span(doc):\n",
        "    noun_chunks = list(doc.noun_chunks)\n",
        "    if noun_chunks:\n",
        "        span = max(noun_chunks, key=lambda c: len(c.text))\n",
        "        return clean_spaces(span.text)\n",
        "    nouns = [t.text for t in doc if t.pos_ in {\"NOUN\",\"PROPN\"} and t.text.lower() not in STOP]\n",
        "    return Counter(nouns).most_common(1)[0][0] if nouns else \"concepto\"\n",
        "\n",
        "def keywords_for(sentence, topn=5):\n",
        "    if YAKE_OK:\n",
        "        try:\n",
        "            kws = [k for k,_ in yake.KeywordExtractor(lan=\"es\", n=1, top=topn).extract_keywords(sentence)]\n",
        "            return list(dict.fromkeys(kws + global_kws))[:max(3, topn)]\n",
        "        except Exception:\n",
        "            pass\n",
        "    # Fallback simple\n",
        "    toks = [t.lower() for t in re.findall(r\"[A-Za-z√Å√â√ç√ì√ö√ú√ë√°√©√≠√≥√∫√º√±]+\", sentence)]\n",
        "    toks = [t for t in toks if t not in STOP and len(t) > 2]\n",
        "    top = [w for w,_ in Counter(toks).most_common(topn)]\n",
        "    return list(dict.fromkeys(top + global_kws))[:max(3, topn)]\n",
        "\n",
        "def build_item(sentence, template_cycle=0):\n",
        "    try:\n",
        "        doc = nlp(sentence)\n",
        "        head = extract_head_span(doc)\n",
        "    except Exception:\n",
        "        head = \"concepto\"\n",
        "    kws = keywords_for(sentence)\n",
        "    idx = template_cycle % 4\n",
        "    if idx == 0: stem,c,d1,d2 = item_definicion(sentence, head)\n",
        "    elif idx == 1: stem,c,d1,d2 = item_cloze(sentence, kws[0] if kws else head)\n",
        "    elif idx == 2: stem,c,d1,d2 = item_excepcion(sentence, head)\n",
        "    else: stem,c,d1,d2 = item_causal(sentence, kws)\n",
        "    target = max(wc(c), wc(d1), wc(d2))\n",
        "    d1, d2 = pad_balance(d1,target), pad_balance(d2,target)\n",
        "    opts = [c,d1,d2]; random.shuffle(opts)\n",
        "    return {\n",
        "        \"stem\": stem,\n",
        "        \"options\": opts,\n",
        "        \"correct_index\": opts.index(c),\n",
        "        \"justification\": \"La opci√≥n correcta conserva el sentido original; los distractores alteran matices o causalidad.\",\n",
        "        \"difficulty\": \"media\",\n",
        "        \"tags\": [\"offline-pro\", f\"spacy:{model_name}\", f\"yake:{YAKE_OK}\"]\n",
        "    }\n",
        "\n",
        "def deduplicate_items(items, max_sim=0.92):\n",
        "    if not items:\n",
        "        return items\n",
        "    stems = [it[\"stem\"] for it in items]\n",
        "    vec = TfidfVectorizer(min_df=1, ngram_range=(1,2)).fit(stems)\n",
        "    keep = []\n",
        "    for i,it in enumerate(items):\n",
        "        si = vec.transform([it[\"stem\"]])\n",
        "        too_sim = any(cosine_similarity(si, vec.transform([items[j][\"stem\"]]))[0,0] > max_sim for j in keep)\n",
        "        if not too_sim: keep.append(i)\n",
        "    return [items[i] for i in keep]\n",
        "\n",
        "# ---------- Generar banco ----------\n",
        "TARGET_ITEMS = 200  # AJUSTA AQU√ç\n",
        "items = []\n",
        "for i,s in enumerate(sents):\n",
        "    it = build_item(s, template_cycle=i)\n",
        "    it[\"id\"] = f\"OFFPRO-{i+1}\"\n",
        "    items.append(it)\n",
        "    if len(items) >= TARGET_ITEMS * 2:\n",
        "        break\n",
        "\n",
        "items = deduplicate_items(items, max_sim=0.92)\n",
        "if len(items) > TARGET_ITEMS:\n",
        "    items = items[:TARGET_ITEMS]\n",
        "\n",
        "print(f\"‚úÖ √çtems OFFLINE PRO generados: {len(items)}\")\n",
        "\n",
        "# ---------- Exportar con tus funciones de la Celda 3 ----------\n",
        "stamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "xml_name = f\"banco_OFFPRO_{len(items)}_{stamp}.xml\"\n",
        "pdf_name = f\"banco_OFFPRO_{len(items)}_{stamp}.pdf\"\n",
        "\n",
        "xml_path = to_moodle_xml(items, xml_path=xml_name)\n",
        "pdf_path = to_pdf(items, pdf_path=pdf_name)\n",
        "print(\"üì¶ XML:\", xml_path)\n",
        "print(\"üìÑ PDF:\", pdf_path)\n",
        "\n",
        "from google.colab import files\n",
        "files.download(xml_path)\n",
        "files.download(pdf_path)\n",
        "\n"
      ],
      "metadata": {
        "id": "R9Na9WTRD9Um"
      },
      "id": "R9Na9WTRD9Um",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}